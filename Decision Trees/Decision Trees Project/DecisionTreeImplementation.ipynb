{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node Class\n",
    "<strong>X</strong> : Part of Training Data this Node has<br>\n",
    "<strong>Y</strong> : Output of part of Training Data this Node has<br>\n",
    "<strong>Features_Unused</strong> : Features that are available for this Node to use<br>\n",
    "<strong>Feature_Selected</strong> : Feature that is used to split this node<br>\n",
    "<strong>Level</strong> : Level of this Node in the decision tree<br>\n",
    "<strong>Children</strong> : Array of all Children this Node has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node :\n",
    "    # Constructor function\n",
    "    def __init__(self, x, y, features_unused, level, features_names) :\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.features_unused = features_unused\n",
    "        self.feature_selected = None\n",
    "        self.level = level\n",
    "        self.features_names = features_names\n",
    "        self.children = []\n",
    "        \n",
    "    # predicts a class for x\n",
    "    def predict(self, x) :\n",
    "        if self.isLeaf() == True :\n",
    "            # return the majority class\n",
    "            max_class = max([(value, key) for key, value in Counter(self.y).items()])[1]\n",
    "            return max_class\n",
    "        \n",
    "        # We will compare the feature of x with the feature that is used to split the Node at this level\n",
    "        feature_used = self.feature_selected\n",
    "        \n",
    "        # Find the index of child in which matches the value of feature\n",
    "        child_index = sorted(set(self.x[:, feature_used])).index(x[feature_used])\n",
    "        \n",
    "        # Recursively ask in that Node\n",
    "        return self.children[child_index].predict(x)\n",
    "            \n",
    "    # find the best feature to split and then build it's subtree\n",
    "    def buildTree(self) :\n",
    "        # check if this node is leaf, if it is then return\n",
    "        if self.isLeaf() == True :\n",
    "            self.printNode()\n",
    "            return \n",
    "        \n",
    "        self.feature_selected = self.getBestFeatureToSplit()  # get best feature to split based on gain ratio\n",
    "        \n",
    "        # Creating Feature_Unused for Children after removing this feature\n",
    "        new_features_unused = self.features_unused.copy()\n",
    "        new_features_unused.remove(self.feature_selected)\n",
    "        \n",
    "        # getting all values this feature can take\n",
    "        feature_classes = set(self.x[:, self.feature_selected])\n",
    "\n",
    "        # splitting the node into multiple children\n",
    "        for feature_class in feature_classes :\n",
    "            x_new = self.x[self.x[:, self.feature_selected] == feature_class]\n",
    "            y_new = self.y[self.x[:, self.feature_selected] == feature_class]\n",
    "            self.children.append(Node(x_new, y_new, new_features_unused.copy(), self.level+1, self.features_names))\n",
    "        \n",
    "        # print Node\n",
    "        self.printNode()\n",
    "        \n",
    "        # build subtrees of children\n",
    "        for child in self.children :\n",
    "            child.buildTree()\n",
    "    \n",
    "    # prints the various details of this node\n",
    "    def printNode(self) :\n",
    "        print('Level : ', self.level)\n",
    "        count = Counter(self.y)\n",
    "        for key in count.keys() :\n",
    "            print('Count of ', key, ' = ', count[key])\n",
    "        print('Current entropy is = ', self.getEntropy())\n",
    "        if self.isLeaf() == True :\n",
    "            print('Reached Leaf Node')\n",
    "            print()\n",
    "            return\n",
    "        print('Splitting on feature number ', self.feature_selected, ' (', self.features_names[self.feature_selected], ') with gain ratio')\n",
    "        print(self.getGainRatio())\n",
    "        print()\n",
    "    \n",
    "    # check if node is leaf or not\n",
    "    def isLeaf(self) :\n",
    "        # node is leaf if all y belong to same class or number of feature_unused = 0\n",
    "        if len(self.features_unused) == 0 :\n",
    "            return True\n",
    "        elif len(set(self.y)) == 1 :\n",
    "            return True\n",
    "        else :\n",
    "            return False\n",
    "        \n",
    "    # gets the best feature to split \n",
    "    def getBestFeatureToSplit(self) :\n",
    "        first = True  # for first feature\n",
    "        \n",
    "        best_feature = None  # best feature that will be selected \n",
    "        best_gain_ratio = None # best gain ratio\n",
    "        \n",
    "        # Trying each feature and choosing the one with best gain ratio\n",
    "        for feature in self.features_unused :\n",
    "            self.feature_selected = feature\n",
    "            self.children = []  # Empty the list of children this node has\n",
    "            \n",
    "            # Creating Feature_Unused for Children after removing this feature\n",
    "            new_features_unused = self.features_unused.copy()\n",
    "            new_features_unused.remove(feature)\n",
    "            \n",
    "            # getting all values this feature can take\n",
    "            feature_classes = set(self.x[:, feature])\n",
    "            \n",
    "            # splitting the node into multiple children\n",
    "            for feature_class in feature_classes :\n",
    "                x_new = self.x[self.x[:, feature] == feature_class]\n",
    "                y_new = self.y[self.x[:, feature] == feature_class]\n",
    "                self.children.append(Node(x_new, y_new, new_features_unused.copy(), self.level+1, self.features_names))\n",
    "            \n",
    "            curr_gain_ratio = self.getGainRatio()\n",
    "            \n",
    "            self.children = []  # empty the children list again\n",
    "            \n",
    "            # compare this gain ratio with gain ratio obtained from other features\n",
    "            if first == True or curr_gain_ratio > best_gain_ratio :\n",
    "                first = False\n",
    "                best_feature = feature\n",
    "                best_gain_ratio = curr_gain_ratio\n",
    "        \n",
    "        return feature  # This is the best feature to split, return it\n",
    "\n",
    "    # get Gain Ratio of this Node on splitting\n",
    "    def getGainRatio(self) :\n",
    "        \n",
    "        # gain ratio = information gain / split info\n",
    "        \n",
    "        info_gain = self.getInformationGain()  # get value of information gain\n",
    "        split_info = self.getSplitInfo()\n",
    "        \n",
    "        return info_gain / split_info\n",
    "    \n",
    "    # get value of information gain for this node on splitting\n",
    "    def getInformationGain(self) :\n",
    "        \n",
    "        # information gain = initial entropy - final entropy\n",
    "        initial_entropy = self.getEntropy()\n",
    "        final_entropy = 0 \n",
    "        for child in self.children :\n",
    "            final_entropy += ((len(child.y)/len(self.y))*child.getEntropy())\n",
    "        \n",
    "        return initial_entropy - final_entropy\n",
    "    \n",
    "    # get split info for this node after splitting\n",
    "    def getSplitInfo(self) :\n",
    "        count = Counter(self.x[:, self.feature_selected])\n",
    "        D = np.array(list(count.values())) / len(self.y)\n",
    "        split_info = -((D*np.log2(D)).sum())\n",
    "        return split_info\n",
    "    \n",
    "    # get entropy of this node\n",
    "    def getEntropy(self) :\n",
    "        output_classes = set(self.y)  # set of all output classes in y\n",
    "        count = Counter(self.y)  # counter to keep a count of various output_classes\n",
    "        p = np.array(list(count.values())) / len(self.y)  # p is an array that stores probabilities of various output_classes\n",
    "        entropy = -((p * np.log2(p)).sum())  # Compute entropy using Vectorization\n",
    "        \n",
    "        return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier Class\n",
    "Now Let's work on the decision tree classifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier :\n",
    "    # Constructor\n",
    "    def __init__(self) :\n",
    "        self.root = None\n",
    "    \n",
    "    # fit function\n",
    "    def fit(self, x_train, y_train, features_names) :\n",
    "        self.root = Node(x_train, y_train, list(range(0,x_train.shape[1])), 0, features_names)\n",
    "        self.root.buildTree()\n",
    "        print()\n",
    "        print('***********************************************************************************')\n",
    "        print('FIT PROCESS COMPLETE')\n",
    "        print('***********************************************************************************')\n",
    "        \n",
    "    # predict function\n",
    "    def predict(self, x_test) :\n",
    "        y_pred = []  # this contains the predicted values\n",
    "        for x in x_test :\n",
    "            y_pred.append(self.root.predict(x))\n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on Iris\n",
    "Let's test out out Decision Tree Classifier on Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "X = datasets.load_iris().data\n",
    "Y = datasets.load_iris().target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the values of iris dataset are continuos, let's label them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels the columns by dividing values into 4 equal parts\n",
    "def getLabeledColumn(column) :\n",
    "    retVal = []\n",
    "    second_cut = column.mean()\n",
    "    first_cut = 0.5*second_cut\n",
    "    third_cut = 1.5*second_cut\n",
    "    for c in column :\n",
    "        if c < first_cut :\n",
    "            retVal.append(0)\n",
    "        elif c < second_cut :\n",
    "            retVal.append(1)\n",
    "        elif c < third_cut :\n",
    "            retVal.append(2)\n",
    "        else :\n",
    "            retVal.append(3)\n",
    "    \n",
    "    return np.array(retVal)\n",
    "\n",
    "for i in range(4) :\n",
    "    X[:, i] = getLabeledColumn(X[:, i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the Dataset into Training and Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit the DecisionTreeClassifier on our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level :  0\n",
      "Count of  2  =  40\n",
      "Count of  0  =  35\n",
      "Count of  1  =  37\n",
      "Current entropy is =  1.5827852442286803\n",
      "Splitting on feature number  3  ( petal width ) with gain ratio\n",
      "0.7523160735955798\n",
      "\n",
      "Level :  1\n",
      "Count of  0  =  35\n",
      "Current entropy is =  -0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level :  1\n",
      "Count of  1  =  5\n",
      "Current entropy is =  -0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level :  1\n",
      "Count of  1  =  31\n",
      "Count of  2  =  5\n",
      "Current entropy is =  0.5813214987637028\n",
      "Splitting on feature number  2  ( petal length ) with gain ratio\n",
      "0.2404641049896386\n",
      "\n",
      "Level :  2\n",
      "Count of  1  =  1\n",
      "Current entropy is =  -0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level :  2\n",
      "Count of  1  =  30\n",
      "Count of  2  =  4\n",
      "Current entropy is =  0.5225593745369408\n",
      "Splitting on feature number  1  ( sepal width ) with gain ratio\n",
      "0.05721590200553489\n",
      "\n",
      "Level :  3\n",
      "Count of  1  =  23\n",
      "Count of  2  =  4\n",
      "Current entropy is =  0.6051865766334206\n",
      "Splitting on feature number  0  ( sepal length ) with gain ratio\n",
      "0.008610038570905617\n",
      "\n",
      "Level :  4\n",
      "Count of  1  =  9\n",
      "Count of  2  =  1\n",
      "Current entropy is =  0.4689955935892812\n",
      "Reached Leaf Node\n",
      "\n",
      "Level :  4\n",
      "Count of  1  =  14\n",
      "Count of  2  =  3\n",
      "Current entropy is =  0.6722948170756379\n",
      "Reached Leaf Node\n",
      "\n",
      "Level :  3\n",
      "Count of  1  =  7\n",
      "Current entropy is =  -0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level :  2\n",
      "Count of  2  =  1\n",
      "Current entropy is =  -0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level :  1\n",
      "Count of  2  =  35\n",
      "Count of  1  =  1\n",
      "Current entropy is =  0.18312206830137276\n",
      "Splitting on feature number  2  ( petal length ) with gain ratio\n",
      "0.022460221014996735\n",
      "\n",
      "Level :  2\n",
      "Count of  2  =  20\n",
      "Count of  1  =  1\n",
      "Current entropy is =  0.2761954276479391\n",
      "Splitting on feature number  1  ( sepal width ) with gain ratio\n",
      "0.07210013773412927\n",
      "\n",
      "Level :  3\n",
      "Count of  2  =  13\n",
      "Current entropy is =  -0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level :  3\n",
      "Count of  1  =  1\n",
      "Count of  2  =  7\n",
      "Current entropy is =  0.5435644431995964\n",
      "Splitting on feature number  0  ( sepal length ) with gain ratio\n",
      "nan\n",
      "\n",
      "Level :  4\n",
      "Count of  1  =  1\n",
      "Count of  2  =  7\n",
      "Current entropy is =  0.5435644431995964\n",
      "Reached Leaf Node\n",
      "\n",
      "Level :  2\n",
      "Count of  2  =  15\n",
      "Current entropy is =  -0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "\n",
      "***********************************************************************************\n",
      "FIT PROCESS COMPLETE\n",
      "***********************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:127: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "features_names = ['sepal length', 'sepal width', 'petal length', 'petal width']\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(x_train, y_train, features_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our Classifier on Test Data and Find Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.97        15\n",
      "           1       0.93      1.00      0.96        13\n",
      "           2       1.00      1.00      1.00        10\n",
      "\n",
      "   micro avg       0.97      0.97      0.97        38\n",
      "   macro avg       0.98      0.98      0.98        38\n",
      "weighted avg       0.98      0.97      0.97        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14  0  0]\n",
      " [ 1 13  0]\n",
      " [ 0  0 10]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We have achieved 98% accuracy. That's LIT"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DecisionTreeImplementation.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
